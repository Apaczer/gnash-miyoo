/**

\page ACT Module ACT: Asynchronous Computation Task

	An ACT is the moral equivalent of a thread in an asynchronous, cooperative, non-preemptive, multitasking environment.
	The name "ACT" is itself an acronym for Asynchronous Computation Task.

\section s_Speed Speed is the Reason

	The whole point of the ACT environment is high performance.
	In comes at a cost, certainly; the tradeoffs are not trivial.
	Preemptive multitasking is easier to implement, but at the cost of the overhead of context switches.
	Each context switch must preserve the entire state of computation, both internal CPU state as well as memory.
	The operating system (or other control facility) is ignorant, perforce, of the details of a computational unit, be it process or thread.
	Thus it cannot optimize a context switch in any way, 
		either by picking a fortuitous point to interrupt (avoiding saving CPU state)
		or by saving less state on the stack than everything (incurring cache and swapping penalties).
	There are two primary benefits of the ACT model.
	The first primary benefit, then, is the elimination of context switch overhead.

	The second benefit is its natural fit with asynchronous and non-blocking I/O.
	Technically, avoiding context switches mandates working within a single thread and using cooperative scheduling.
	Cooperative scheduling means that a task must manually yield its control of the CPU back to its environment.
	The natural time to do that is when receiving a "would block" return on a non-blocking I/O call.
	Every program that uses non-blocking I/O needs to be able to resume itself at the effective point where the computation left off.
	In the ACT system, this is simply a requirement for all tasks, so doing it for non-blocking I/O adds no extra overhead.
	The implementation pattern that comes to the fore is the "state of a partial computation".
	One element of a partial computation state is a record of what I/O operation has been set to operate asynchronously.

\section s_ACT_Elements Elements of the ACT Environment
	The main elements of the ACT environment the following:
	- \b Actions.  
		These are ACT classes that do all the work; it's the equivalent of a thread body.
		The ultimate purpose of the ACT environment is to provide an execution context for actions.
		An action runs until it yields (typically because it would block).
		At such a time it enters a waiting state, waiting to be awakened as a result of further activity.
		
	- \b Scheduler.
		In preemptive multitasking environments, the scheduling function is provided transparently.
		In the ACT environment, ordinary actions see scheduling as transparent,
			but the scheduler is visible to certain primitives, particularly those involved with I/O.
		A scheduler is single-threaded and is designed to be the body of a thread.
		The scheduler, however, is not a singleton, as the preferred deployment configuration is one-scheduler-per-CPU.
		
	- \b Demons.
		Demons are a category of actions that run persistently in the scheduler.
		The scheduler views ordinary actions as tasks, which are expected to eventually terminate.
		The scheduler has different execution policies for demons and tasks.
		The two most significant kinds of demons are services and monitors.
		
	- \b Services.
		A service is a demon that generates new tasks.
		A service typically represents some kind of bound input device, activity at which needs some response.
		The response to such input activity is a new task.
		For example, a socket listening at port 80 generates a new HTTP protocol task for each incoming connection. 
		As a rule, services represent input response, but not as a requirement.
		A service may generate new tasks based on purely internal considerations as well.
		
	- \b Monitors.
		A monitor is a demon that wakes up tasks from their waiting state.
		A task, once it yields to wait for new activity, won't be scheduled until something else rouses it.
		For each waiting task, there is some monitor demon that has responsibility for waking it up.
		Sensing the activity that triggers a wake-up is domain-specific,
			so there's a separate monitor class for each kind of waiting, in particular each kind of I/O.
			
	- <b>Listener Actions</b>.
		The code for actions is recursive, one action calling another as needed.
		This is as for ordinary code, with the distinction that the action invariants must be preserved at each call.
		A listener action is a primitive action that calls no other action and cooperates with a monitor for wake-up.
		As a result, each leaf in the call graph of an action is either a listener action or one that spontaneously yields.

\section s_Quiescence Quiescence

	What does the scheduler do when it has no task work that needs to be done?
	If it also has no demons running, it simply exits.
	If it does have demons running, it activates them round-robin until they all terminate or generate new tasks.
	In the first approximation, then, the scheduler will consume all available CPU unless somehow throttled.
	The basic tactic to throttle a quiescent scheduler is to have a demon running that blocks.
	This is the only place in the system where blocking is an ordinary part of the architecture.

	Now just to be contradictory, consider the following example.
	The scheduler is running in a dedicated device where power consumption due to CPU usage isn't an issue.
	Using 100% CPU is no problem here.
	So a blocking demon for quiescent operation isn't necessary in this case.
	The example shows that blocking is used during quiescent operation to be kind to some larger execution environment.

	So suppose that the scheduler is running some demon that can block.
	How that demon unblocks is the real issue.
	The easiest way to unblock generates a polling algorithm: it simply waits some amount of time, then unblocks.
	An implementation of this idea is the class ACT::Pause_Demon.
	Without the motivation to properly handle quiescent operation, it's a bit of a mystery why there should be a task that just eats time.

	\b Note: Interrupt I/O is not yet implemented.
	It's on the timeline.

	The other way of unblocking is by interrupt.
	With this method, actions waiting for asynchronous I/O receive notifications on another thread.
	This notification triggers unblocking in the demon that's blocked in the same same thread as the waiting action.
	It's desirable that all interruptions from any source converge upon a single demon that aggregates them.
	The alternatives require a hybrid algorithm with polling that doesn't perform well.

	These two strategies each have advantageous niches.
	The downside to a polling strategy is that under quiescence it consumes some constant fraction of CPU
		and has a certain average latency in its responses, proportional to that responsiveness.
	Its upside is that under load it does no work over and above what it would have do anyway.
	The downside of an interrupt stategy is that there's task-switching overhead in the interrupt process.
	Its upside is that under quiescence it has zero CPU overhead and good latency response.
	The hybrid strategy is to switch between these two depending on load.

	From a development sequence point of view, however, polling is easy and interrupt is hard.
	Thus, polling was done at the outset.
	As of this writing, interrupting is not implemented.
	Since the first use case of the scheduler is for deployment on dedicated server hardware, 
		this absence has not become problematic.
	Once the ACT environment is used on client software, however, this will have to change.

	The ACT architecture enables the separation of polling-vs.-interrupt strategy by using listener actions and monitors.
	While the underlying I/O calls may be identical, there are different control structures around them for the two strategies.
	Once both are implemented, however, swapping between them is a matter of link-time configuration.
	Implementing a hybrid strategy requires writing more effort, but it too is simply linked in.
	As a result, there's little project risk to delaying the interrupt and hybrid strategies.

\section s_Shutdown Shutdown

	\b Note: The facilities in this section are not yet implemented for the most part.

	Shutting down a server is an important use case for system administrators.
	They have different reasons for shutting down a process, ranging from emergency ("Process is a security breach!") to the languid ("Regular version upgrade.").
	The ACT environment takes the basic stance that shutdown is application-specific.
	The application defines a class representing a shutdown order,
		and it's a parameter to both the scheduler and to actions.
	The author of an action class is responsible for shutting them down in whatever way they may find appropriate
		and documenting that behavior.
	There remains the issue of what the scheduler does.

	To start with, there's always the option of killing it pre-emptively.
	It's not graceful, but it's available.
	So let's assume that the application wishes to cooperate with the scheduler.

	A scheduler must determine what to do with the actions in its scheduling queue when it receives a shutdown order.
	The simplest, though rarely the best, thing to do is nothing.
	It is able to simply cease processing altogether and return from its main loop with all its items left intact.
	Up to this stage, this is an order to suspend.
	Now, if the thread main function were a scheduler's execution loop (as it supports),
		to continue to do nothing means that the thread terminates and presumably then the scheduler goes out of scope.

	There's also a slightly more aggressive	version of nothing, 
		which is to remove all references to a scheduled item from the scheduling queue.
	In the typical case, the only reference to an item is within the queue (think tasks generated by a service)
		and so this causes such an object to go out of scope immediately.
	Removing such a reference is the regular policy if an action completes itself normally.
	This removal is unusual because it happens when an action is still in an incomplete state.	

	If shutdown is not required immediately, then the scheduler may continue to execute actions
		in anticipating of termination, presumably in the near future.
	The next question is how many times to perform such execution.
	As the proverbial angel-on-pinheads, the choice is between a definite and an indefinite bound.
	If a definite execution bound is desired, it might be by execution count or by a time limit (or both).

	Summarizing, we have the following shutdown algorithm, with its main points of configuration:
	- Decide upon a cooperative shutdown.
	- Construct a shutdown parameter and invoke the scheduler's shutdown function with it.
	- If the scheduler is immediately suspending, do so now, deleting references if requested.
	- Call the shutdown interface on each action in the scheduling queue.
	- If the scheduler has a zero execution bound (either by count or in time), return now, deleting references if requested.
	- If execution bounds are not in place, resume ordinary operations.  The actions will shut themselves down.
	- If execution bound are in place, continue with bounded operation.  When an action overreaches a bound, delete it if requested.

\section s_Implementation_Tactics Implementation Tactics

	- <b>Scheduler-specific Storage</b>.
		It is permitted to have one monitor scheduled for each waiting task.
		In an I/O-bound environment, most tasks at any given time are waiting for I/O,
			so a one-monitor-per-waiting-task policy almost doubles the size of the scheduling queue.
		To alleviate this issue, scheduler-specific storage enables a single monitor for all waiting tasks of the same category.
		This works by allowing a listener action to navigate to the correct monitor for its scheduler.
		Using one monitor per scheduler rather than one monitor per system eliminates
			what would otherwise be a large number of cross-thread messages, requiring relatively expensive mutex protection.
 */
